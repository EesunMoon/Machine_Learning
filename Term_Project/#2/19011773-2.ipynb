{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "posted-corporation",
   "metadata": {
    "papermill": {
     "duration": 0.017754,
     "end_time": "2021-06-06T13:19:44.381918",
     "exception": false,
     "start_time": "2021-06-06T13:19:44.364164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 반드시 처음부터 끝까지 스켈레톤 코드를 살펴보고 구현하기 시작하길 바란다\n",
    "\n",
    "## 1. 스켈레톤 코드를 [복사 및 편집] 하여 사용한다.\n",
    "## 2. 아래의 [Empty Module 3개]를 직접 구현한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "anticipated-stanley",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:44.418721Z",
     "iopub.status.busy": "2021-06-06T13:19:44.417486Z",
     "iopub.status.idle": "2021-06-06T13:19:44.429481Z",
     "shell.execute_reply": "2021-06-06T13:19:44.430240Z",
     "shell.execute_reply.started": "2021-06-06T13:05:30.858042Z"
    },
    "papermill": {
     "duration": 0.032722,
     "end_time": "2021-06-06T13:19:44.430952",
     "exception": false,
     "start_time": "2021-06-06T13:19:44.398230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/2021-ml-tp2-spam/train.csv\n",
      "/kaggle/input/2021-ml-tp2-spam/test.csv\n",
      "/kaggle/input/2021-ml-tp2-spam/sample_submit.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "understood-winter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:44.469139Z",
     "iopub.status.busy": "2021-06-06T13:19:44.468395Z",
     "iopub.status.idle": "2021-06-06T13:19:46.139017Z",
     "shell.execute_reply": "2021-06-06T13:19:46.138278Z",
     "shell.execute_reply.started": "2021-06-06T13:05:30.877659Z"
    },
    "papermill": {
     "duration": 1.690196,
     "end_time": "2021-06-06T13:19:46.139160",
     "exception": false,
     "start_time": "2021-06-06T13:19:44.448964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-offering",
   "metadata": {
    "papermill": {
     "duration": 0.01615,
     "end_time": "2021-06-06T13:19:46.172044",
     "exception": false,
     "start_time": "2021-06-06T13:19:46.155894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- nltk 라이브러리에서 punkt 데이터를 다운 받음, 이 데이터는 영화 리뷰와 같은 문서 데이터로 문자의 tokeninizer를 위해서 필요하다\n",
    "- nltk 라이브러리를 이용해서 불용어를 다운 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "outdoor-community",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:46.217923Z",
     "iopub.status.busy": "2021-06-06T13:19:46.217204Z",
     "iopub.status.idle": "2021-06-06T13:19:46.617752Z",
     "shell.execute_reply": "2021-06-06T13:19:46.618248Z",
     "shell.execute_reply.started": "2021-06-06T13:05:31.451216Z"
    },
    "papermill": {
     "duration": 0.429863,
     "end_time": "2021-06-06T13:19:46.618416",
     "exception": false,
     "start_time": "2021-06-06T13:19:46.188553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sized-preservation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:46.656212Z",
     "iopub.status.busy": "2021-06-06T13:19:46.655543Z",
     "iopub.status.idle": "2021-06-06T13:19:46.706269Z",
     "shell.execute_reply": "2021-06-06T13:19:46.706762Z",
     "shell.execute_reply.started": "2021-06-06T13:05:31.622235Z"
    },
    "papermill": {
     "duration": 0.071333,
     "end_time": "2021-06-06T13:19:46.706945",
     "exception": false,
     "start_time": "2021-06-06T13:19:46.635612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       No I'm in the same boat. Still here at my moms...\n",
       "1       (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
       "2          They r giving a second chance to rahul dengra.\n",
       "3          O i played smash bros  &lt;#&gt;  religiously.\n",
       "4       PRIVATE! Your 2003 Account Statement for 07973...\n",
       "                              ...                        \n",
       "4452    I came hostel. I m going to sleep. Plz call me...\n",
       "4453                               Sorry, I'll call later\n",
       "4454        Prabha..i'm soryda..realy..frm heart i'm sory\n",
       "4455                           Nt joking seriously i told\n",
       "4456                  In work now. Going have in few min.\n",
       "Name: Data, Length: 4457, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"/kaggle/input/2021-ml-tp2-spam/train.csv\",encoding=\"latin-1\")\n",
    "df_test = pd.read_csv(\"/kaggle/input/2021-ml-tp2-spam/test.csv\",encoding=\"latin-1\")\n",
    "df_train.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fatal-reggae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:46.746017Z",
     "iopub.status.busy": "2021-06-06T13:19:46.745048Z",
     "iopub.status.idle": "2021-06-06T13:19:46.749764Z",
     "shell.execute_reply": "2021-06-06T13:19:46.750213Z",
     "shell.execute_reply.started": "2021-06-06T13:05:31.660190Z"
    },
    "papermill": {
     "duration": 0.025781,
     "end_time": "2021-06-06T13:19:46.750392",
     "exception": false,
     "start_time": "2021-06-06T13:19:46.724611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = df_train[\"Data\"]\n",
    "y_train=df_train[\"Class\"]\n",
    "X_test=df_test[\"Data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-omega",
   "metadata": {
    "papermill": {
     "duration": 0.017217,
     "end_time": "2021-06-06T13:19:46.786009",
     "exception": false,
     "start_time": "2021-06-06T13:19:46.768792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [Empty Module #1] 텍스트 데이터 전처리  \n",
    "\n",
    "목표: 텍스트 데이터를 처리하기 위한 여러 과정을 거쳐, 머신을 위한 데이터를 만든다. \n",
    "\n",
    "```\n",
    "[input]\n",
    "--------------\n",
    "- text: 텍스트 문장 데이터 \n",
    "\n",
    "[output]\n",
    "--------------\n",
    "- text: 전처리를 완료한 문장 데이터 \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alike-burke",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:46.824954Z",
     "iopub.status.busy": "2021-06-06T13:19:46.823926Z",
     "iopub.status.idle": "2021-06-06T13:19:46.833346Z",
     "shell.execute_reply": "2021-06-06T13:19:46.832836Z",
     "shell.execute_reply.started": "2021-06-06T13:05:31.666513Z"
    },
    "papermill": {
     "duration": 0.030239,
     "end_time": "2021-06-06T13:19:46.833664",
     "exception": false,
     "start_time": "2021-06-06T13:19:46.803425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# [Empty Module #1] 텍스트 데이터 전처리\n",
    "# ------------------------------------------------\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def data_processing(text):\n",
    "    # ------------------------------------------------------------\n",
    "    # 구현 가이드라인 \n",
    "    # ------------------------------------------------------------\n",
    "    # [1] re.sub 사용해 text 속 '[^A-Za-z]' 외의 문자만을 찾아내 제거한후, pre_words 변수에 저장\n",
    "    #      1) pattern은 '[^A-Za-z]', repl=' ' 로 각각 설정.\n",
    "    #      2) 이모지나 숫자,점과 같은 문자외의 것들을 제거했다. (이모지는 감정 분석과 관련해서 몇가지 의미를 나타내지만 이 테스크에서는 이러한 의미도 제거함.)\n",
    "    #\n",
    "    # [2] pre_words의 lower 내장 함수를 이용해 대문자들은 소문자로 변경\n",
    "    #      1)  대, 소문자가 구분되어 있으면 \"Go\"와 \"go\" 와 같이 동일한 단어를 머신은 다른 단어로 취급한다. 따라서 대문자를 모두 소문자로 변경.\n",
    "    #\n",
    "    # [3] word_tokenize 함수를 이용해 pre_word 를 토큰화하여 word를 리스트화한 후 tokenized_words변수에 저장\n",
    "    #\n",
    "    # [4] nltk 라이브러리로 다운 받은 stopwords의 \"words\" 내장 함수를 이용해 english 불용어를 찾아서 stops 변수에 저장  \n",
    "    #      1) 불용어: 텍스트 분류에서 불용어는 텍스트의 중요도을 결정하는데 영향을 미치지 않는 단어임. \n",
    "    #                    (ex: the, we, a , will), 따라서 불필요한 단어가 예측 모델에 악영향을 끼칠 수 있기 때문에 제거.\n",
    "    #\n",
    "    # [5] [3] 에서 찾은 문자열 중 단어가 [4] 에서 찾은 불용어 속에 없을 경우, tokenized_words_remove 리스트에 append \n",
    "    #\n",
    "    # [6] PorterStemmer 속 stem 내장 함수를 이용해 동일 의미를 갖는 단어를 동일한 단어로 변경하는 과정을 거친 후 다시 저장.\n",
    "    #    \n",
    "    # ------------------------------------------------------------\n",
    "    ##############\n",
    "    \n",
    "    # [1] 대문자나 소문자가 아닌 문자들을 제거하여 pre_words 변수에 저장\n",
    "    pre_words = re.sub('[^A-Za-z]', ' ' , text) \n",
    "    # re.sub('패턴', 교체함수, '문자열')\n",
    "    # 대문자나 소문자가 아닌 문자들을 나타내는 정규표현식 : [A-Za-z] => 패턴\n",
    "    # 교체함수 : ' ', 문자열 : text\n",
    "    ##############\n",
    "   \n",
    "    # [2] pre_words내의 대문자들을 소문자로 변경\n",
    "    pre_words = pre_words.lower() # .lower() 함수를 이용하여 대문자를 소문자로 변경\n",
    "    ##############\n",
    "    \n",
    "    # [3] word_tokenize 함수를 이용해 pre_word 를 토큰화하여 word를 리스트화한 후 tokenized_words변수에 저장\n",
    "    tokenized_words = word_tokenize(pre_words) # word_tokenize 함수 : space 단위와 구두점(punctuation)을 기준으로 토큰화(Tokenize)하는 함수\n",
    "    ##############\n",
    "\n",
    "    # [4] nltk 라이브러리로 다운 받은 stopwords의 \"words\" 내장 함수를 이용해 english 불용어를 찾아서 stops 변수에 저장 \n",
    "    stops = stopwords.words('english') # .words('english') : stopwords 안의 english 불용어를 찾기\n",
    "    \n",
    "    ##############\n",
    "    tokenized_words_remove=[] # 불용어 속에 없는 단어를 저장하는 리스트\n",
    "    \n",
    "    for w in tokenized_words: \n",
    "        # [5] [3] 에서 찾은 문자열 중 단어가 [4] 에서 찾은 불용어 속에 없을 경우, tokenized_words_remove 리스트에 append \n",
    "        if w not in stops: # stops 속에 tokenized_words의 단어(w)가 없을 경우\n",
    "                       tokenized_words_remove.append(w) # w를 리스트에 추가\n",
    "        ############## \n",
    "    \n",
    "    stemmer = PorterStemmer() # PorterStemmer()를 stemmer에 저장\n",
    "    # 중요도에 결정을 미치지 않는 단어인 불용어를 제외한 단어들을 저장한 리스트인 tokenized_words_remove를 사용해야함!!\n",
    "    for i in range(len(tokenized_words_remove)):\n",
    "        # [6] PorterStemmer 속 stem 내장 함수를 이용해 동일 의미를 갖는 단어를 동일한 단어로 변경하는 과정을 거친 후 다시 저장.\n",
    "        tokenized_words_remove[i] = stemmer.stem(tokenized_words_remove[i]) # Porter stemming algorithm : 영어의 접미사(suffix)를 제거\n",
    "        ##############\n",
    "    \n",
    "    return ( \" \".join( tokenized_words_remove ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electric-morrison",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:46.874664Z",
     "iopub.status.busy": "2021-06-06T13:19:46.873839Z",
     "iopub.status.idle": "2021-06-06T13:19:50.537508Z",
     "shell.execute_reply": "2021-06-06T13:19:50.536804Z",
     "shell.execute_reply.started": "2021-06-06T13:05:31.683376Z"
    },
    "papermill": {
     "duration": 3.686217,
     "end_time": "2021-06-06T13:19:50.537699",
     "exception": false,
     "start_time": "2021-06-06T13:19:46.851482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train=[data_processing(i) for i in X_train]\n",
    "X_test=[data_processing(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-coverage",
   "metadata": {
    "papermill": {
     "duration": 0.017947,
     "end_time": "2021-06-06T13:19:50.573705",
     "exception": false,
     "start_time": "2021-06-06T13:19:50.555758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [Empty Module #2] Bag of Word \n",
    "\n",
    "목표: 문장 데이터를 머신을 학습하기 위한 실수화된 Feature로 변경하기로한다. \n",
    "\n",
    "- train 과 test 데이터 전부 type을 ('U')로 변경하여 Countvectorizer를 사용한다. \n",
    "- [설명 링크](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proper-tuning",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:50.623111Z",
     "iopub.status.busy": "2021-06-06T13:19:50.621921Z",
     "iopub.status.idle": "2021-06-06T13:19:50.712140Z",
     "shell.execute_reply": "2021-06-06T13:19:50.711617Z",
     "shell.execute_reply.started": "2021-06-06T13:05:35.354212Z"
    },
    "papermill": {
     "duration": 0.120984,
     "end_time": "2021-06-06T13:19:50.712287",
     "exception": false,
     "start_time": "2021-06-06T13:19:50.591303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hashing vectorizer- Tfidf vectorizer가 성능측면으로 더욱 뛰어나서 Tfidf vectorizer를 사용하였습니다 => [[baseline 1]]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# [Empty Module #2] 텍스트 데이터 Bag of word  feature  화 \n",
    "# ------------------------------------------------\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# ------------------------------------------------------------\n",
    "# 구현 가이드라인 \n",
    "# ------------------------------------------------------------\n",
    "# [1]  CountVectorizer를 정의 \n",
    "#           1) max_features를 100으로 지정 \n",
    "# [2] X_train 과 X_test를 numpy array로 변환 후 데이터 타입을 \"U\"로 변경해 저장\n",
    "#\n",
    "# [3] CountVectorizer를 이용해 X_train은 학습 및 변환을 하고, X_test는 변환을 진행. \n",
    "# ------------------------------------------------------------\n",
    "###########\n",
    "\n",
    "# # [1]\n",
    "vectorizer = HashingVectorizer() # HashingVectorizer 모듈 초기화\n",
    "             \n",
    "# [2] X_train과 X_test를 np.array형태로 변환한 후 데이터 타입을 전부 'U'로 변경\n",
    "X_train = np.asarray(X_train, dtype = 'U')\n",
    "X_test = np.asarray(X_test, dtype = 'U')\n",
    "\n",
    "# [3] HashingVectorizer로 x_train를 학습 및 변환하고, x_test를 변환\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sustainable-welding",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:50.762266Z",
     "iopub.status.busy": "2021-06-06T13:19:50.756712Z",
     "iopub.status.idle": "2021-06-06T13:19:50.970797Z",
     "shell.execute_reply": "2021-06-06T13:19:50.970247Z",
     "shell.execute_reply.started": "2021-06-06T13:05:35.449465Z"
    },
    "papermill": {
     "duration": 0.241048,
     "end_time": "2021-06-06T13:19:50.970950",
     "exception": false,
     "start_time": "2021-06-06T13:19:50.729902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count vectorizer- Tfidf vectorizer가 성능측면으로 더욱 뛰어나서 Tfidf vectorizer를 사용하였습니다 => [[baseline 3]]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# [Empty Module #2] 텍스트 데이터 Bag of word  feature  화 \n",
    "# ------------------------------------------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# ------------------------------------------------------------\n",
    "# 구현 가이드라인 \n",
    "# ------------------------------------------------------------\n",
    "# [1]  CountVectorizer를 정의 \n",
    "#           1) max_features를 100으로 지정 \n",
    "# [2] X_train 과 X_test를 numpy array로 변환 후 데이터 타입을 \"U\"로 변경해 저장\n",
    "#\n",
    "# [3] CountVectorizer를 이용해 X_train은 학습 및 변환을 하고, X_test는 변환을 진행. \n",
    "# ------------------------------------------------------------\n",
    "###########\n",
    "\n",
    "# [1]\n",
    "vectorizer = CountVectorizer(max_features = 100) # 실수 데이터를 범주형 데이터로 바꾸기\n",
    "             \n",
    "# [2] X_train과 X_test를 np.array형태로 변환한 후 데이터 타입을 전부 'U'로 변경\n",
    "X_train = np.asarray(X_train, dtype = 'U')\n",
    "X_test = np.asarray(X_test, dtype = 'U')\n",
    "\n",
    "# [3] CountVectorizer로 x_train를 학습 및 변환하고, x_test를 변환\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "harmful-tribute",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:51.052389Z",
     "iopub.status.busy": "2021-06-06T13:19:51.046934Z",
     "iopub.status.idle": "2021-06-06T13:19:51.125602Z",
     "shell.execute_reply": "2021-06-06T13:19:51.125048Z",
     "shell.execute_reply.started": "2021-06-06T13:05:35.669779Z"
    },
    "papermill": {
     "duration": 0.137079,
     "end_time": "2021-06-06T13:19:51.125756",
     "exception": false,
     "start_time": "2021-06-06T13:19:50.988677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tfidf Vectorizer => [[baseline 2]]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# [Empty Module #2] 텍스트 데이터 Bag of word  feature  화 \n",
    "# ------------------------------------------------\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# ------------------------------------------------------------\n",
    "# 구현 가이드라인 \n",
    "# ------------------------------------------------------------\n",
    "# [1]  CountVectorizer를 정의 \n",
    "#           1) max_features를 100으로 지정 \n",
    "# [2] X_train 과 X_test를 numpy array로 변환 후 데이터 타입을 \"U\"로 변경해 저장\n",
    "#\n",
    "# [3] CountVectorizer를 이용해 X_train은 학습 및 변환을 하고, X_test는 변환을 진행. \n",
    "# ------------------------------------------------------------\n",
    "###########\n",
    "\n",
    "# [1] TfidfVectorizer를 정의\n",
    "vectorizer = TfidfVectorizer(max_features = 9000) # max_feature : 용어 빈도에 의해 정렬된 상위 max_features만 고려하는 어휘를 구축\n",
    "# max_feature를 높여보며 성능 향상을 시켜보고자 하였습니다\n",
    "# max_feature = 1000 => 제출 성능 : 0.97937\n",
    "# max_feature = 3000 => 제출 성능 : 0.98116\n",
    "# max_feature = 5000 => 제출 성능 : 0.98295\n",
    "# max_feature = 7000 => 제출 성능 : 0.98475\n",
    "# max_feature = 9000 => 제출 성능 : 0.98475 => 선택!\n",
    "             \n",
    "# [2] train data와 test data의 데이터 타입을 U(=Unicode string)로 변경하고 np.array로 변환\n",
    "X_train = np.asarray(X_train, dtype = 'U')\n",
    "X_test = np.asarray(X_test, dtype = 'U')\n",
    "\n",
    "# [3] train data로 학습 및 변환하고, train data로 학습한 것을 이용하여 test data를 변환\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "opponent-authority",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:51.174450Z",
     "iopub.status.busy": "2021-06-06T13:19:51.173379Z",
     "iopub.status.idle": "2021-06-06T13:19:51.176219Z",
     "shell.execute_reply": "2021-06-06T13:19:51.176736Z",
     "shell.execute_reply.started": "2021-06-06T13:05:35.782055Z"
    },
    "papermill": {
     "duration": 0.03352,
     "end_time": "2021-06-06T13:19:51.176942",
     "exception": false,
     "start_time": "2021-06-06T13:19:51.143422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train[y_train==\"ham\"]=0\n",
    "y_train[y_train==\"spam\"]=1\n",
    "y_train=y_train.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-empty",
   "metadata": {
    "papermill": {
     "duration": 0.017322,
     "end_time": "2021-06-06T13:19:51.212171",
     "exception": false,
     "start_time": "2021-06-06T13:19:51.194849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## [Empty Module #3] SVM: classifier\n",
    "목표: SVC() 를 활용해 classification 을 진행\n",
    "\n",
    "- fit()으로 train 에 대한 머신러닝 학습\n",
    "- predict()으로 test 에 대한 정답을 추론 하여 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prompt-stopping",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:51.258913Z",
     "iopub.status.busy": "2021-06-06T13:19:51.258074Z",
     "iopub.status.idle": "2021-06-06T13:19:54.487599Z",
     "shell.execute_reply": "2021-06-06T13:19:54.488107Z",
     "shell.execute_reply.started": "2021-06-06T13:19:14.693361Z"
    },
    "papermill": {
     "duration": 3.258244,
     "end_time": "2021-06-06T13:19:54.488282",
     "exception": false,
     "start_time": "2021-06-06T13:19:51.230038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9991025353376711\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------\n",
    "# [Empty Module #3] 텍스트 데이터 Bag of word  feature  화 \n",
    "# ------------------------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# ------------------------------------------------------------\n",
    "# 구현 가이드라인 \n",
    "# ------------------------------------------------------------\n",
    "# [1]  SVC 정의 \n",
    "#           1) gamma=\"auto\" 사용 \n",
    "# [2] X_train_features과 y_train으로 SVC 학습진행 후, X_test_features로 predict 진행\n",
    "#\n",
    "# ------------------------------------------------------------\n",
    "###########\n",
    "# [1]\n",
    "\n",
    "# grid search로 최적 모델 찾기 << 파라미터를 조정하는 데 척도로 사용하였습니다\n",
    "# param = {'C' : [1000, 2000, 3000, 4000, 5000]} # grid search의 파라미터 지정\n",
    "\n",
    "## svc\n",
    "svc = SVC(gamma = 'auto', kernel = 'rbf', class_weight = 'balanced', C = 10100, probability=True) \n",
    "# svc 정의 : 파라미터를 바꿔보며 제출성능을 측정해 본 결과 중 가장 좋은 파라미터로 초기화\n",
    "\n",
    "# clf = GridSearchCV(svc, param, n_jobs = -1) # grid search 정의 => svc 파라미터를 조정하는 과정에서 사용하였습니다.\n",
    "\n",
    "## SVC 성능 비교 ##\n",
    "# param = {'C' : [0.001, 0.1, 1, 10, 100, 1000]}\n",
    "# 위 파라미터로 C값을 조정해보았을 때, \n",
    "# grid search best score : 0.9807044032754048\n",
    "# best_params_ : {'C': 100}\n",
    "# 제출 성능 : 0.98206\n",
    "# 로 C값이 클수록 성능이 좋아짐을 알 수 있었습니다.\n",
    "\n",
    "# 따라서 param = {'C' : [1000, 2000, 3000, 4000, 5000]} 로 파라미터를 수정하고 grid search를 해보았을 때\n",
    "# grid search best score : 0.9822744132908557\n",
    "# best_params_ : {'C': 4000}\n",
    "# 제출 성능 : 0.98295\n",
    "# 을 보고 C값을 더 키워보았습니다.\n",
    "\n",
    "# C = 7000 => 제출 성능 : 0.98295 << C = 4000이었을 때와 동일\n",
    "# C = 10000 => 제출 성능 : 0.98475 << 최고 성능..!\n",
    "# C = 12000 => 제출 성능 : 0.98206 << C = 10000이었을 때보다 떨어짐\n",
    "# C = 11000 => 제출 성능 : 0.98206 << C = 12000이었을 때와 동일\n",
    "# C = 10010 => 제출 성능 : 0.98475 << C = 10000이었을 때와 동일 \n",
    "# C = 9000 => 제출 성능 : 0.98295\n",
    "# C = 10100 => 제출 성능 : 0.98475\n",
    "# C = 9500 => 제출 성능 : 0.98295\n",
    "## 위 처럼 C값을 조정해보며 성능을 비교해본 결과 C값이 10000~10100 일 때 가장 좋은 성능을 보였고, 10100보다 큰 C값이나, 10000보다 낮은 C값을 가질 때 성능이 낮아짐을 알 수 있었습니다.\n",
    "\n",
    "# 추가적인 성능 향상을 위해 empty module #2에서 max_feature 값을 높여보며 제출 성능 향상 추이를 보았고, max_feature가 7000과 9000일 때 같은 성능을 보임을 확인하였습니다.\n",
    "# max_feature를 9000으로 유지한 채 SVC의 C파라미터를 조정해보며 성능 향상 추이를 보았습니다.\n",
    "## max_feature = 9000일 때\n",
    "# C = 10100 => 제출 성능 : 0.98475 ==>> best # .score : 0.9991025353376711\n",
    "# C = 15000 => 제출 성능 : 0.98385\n",
    "# C = 20000 => 제출 성능 : 0.98295\n",
    "# C = 5000 => 제출 성능 : 0.98295\n",
    "# C = 30000 => 제출 성능 : 0.98385\n",
    "# C = 28000 => 제출 성능 : 0.98385\n",
    "# C = 25000 => 제출 성능 : 0.98385\n",
    "\n",
    "## random forest\n",
    "rfc = RandomForestClassifier(random_state = 1, n_estimators = 50)\n",
    "# rfc \n",
    "# default => 제출성능 : 0.97937\n",
    "# criterion = 'entropy', n_estimators = 200 => 제출성능 : 0.97757\n",
    "# n_estimators = 500 => 제출성능 : 0.97937\n",
    "# n_estimators = 1000 => 제출성능 : 0.97937\n",
    "# n_estimators = 50 => 제출성능 : 0.97937\n",
    "\n",
    "## gradient boosting\n",
    "boosting = GradientBoostingClassifier(random_state = 1, n_estimators = 1000)\n",
    "# 제출성능 : 0.97309\n",
    "\n",
    "## adaboost\n",
    "ada = AdaBoostClassifier(random_state = 1, n_estimators = 500)\n",
    "# n_estimators = 500 => 0.98026\n",
    "# n_estimators = 1000 => 0.97937\n",
    "# n_estimators = 300 => 0.97847\n",
    "\n",
    "## bagging\n",
    "bagging = BaggingClassifier(random_state = 1, n_estimators = 1500)\n",
    "# 제출성능 : 0.97040\n",
    "\n",
    "## voting\n",
    "eclf = VotingClassifier(estimators=[('svc', svc), ('ada', ada)], voting='soft')\n",
    "# rfc, svc, boosting => .score : 0.9995512676688355\n",
    "# rfc, svc, ada => .score : 0.9997756338344178 , 제출성능 : 0.98475\n",
    "\n",
    "## 성능 향상을 위해 위의 앙상블 모델을 사용해보았지만, svc 모델 하나만 쓰는 것이 가장 성능이 잘 나왔습니다.\n",
    "\n",
    "# [2] X_train_features과 y_train으로 SVC 학습진행 후, X_test_features로 predict 진행\n",
    "svc.fit(X_train_features, y_train) # train data로 svc 학습\n",
    "print(svc.score(X_train_features, y_train))\n",
    "\n",
    "# grid search로 진행했을 때\n",
    "# best_model = clf.best_estimator_ # best_model에 grid search로 찾은 최적의 모델 저장\n",
    "# print(clf.best_score_)\n",
    "# print(clf.best_params_)\n",
    "\n",
    "# test data에 대한 예측값 저장\n",
    "y_pred = svc.predict(X_test_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-camcorder",
   "metadata": {
    "papermill": {
     "duration": 0.017216,
     "end_time": "2021-06-06T13:19:54.523183",
     "exception": false,
     "start_time": "2021-06-06T13:19:54.505967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "moderate-generic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T13:19:54.564730Z",
     "iopub.status.busy": "2021-06-06T13:19:54.564008Z",
     "iopub.status.idle": "2021-06-06T13:19:54.572515Z",
     "shell.execute_reply": "2021-06-06T13:19:54.572008Z",
     "shell.execute_reply.started": "2021-06-06T13:19:20.627017Z"
    },
    "papermill": {
     "duration": 0.031904,
     "end_time": "2021-06-06T13:19:54.572673",
     "exception": false,
     "start_time": "2021-06-06T13:19:54.540769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred={\"ID\": range(np.array(y_pred).shape[0]),\"Class\":y_pred}\n",
    "df_pred=pd.DataFrame(df_pred)\n",
    "df_pred.to_csv(\"predict.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-promise",
   "metadata": {
    "papermill": {
     "duration": 0.017569,
     "end_time": "2021-06-06T13:19:54.608084",
     "exception": false,
     "start_time": "2021-06-06T13:19:54.590515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-final",
   "metadata": {
    "papermill": {
     "duration": 0.017456,
     "end_time": "2021-06-06T13:19:54.643384",
     "exception": false,
     "start_time": "2021-06-06T13:19:54.625928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.630596,
   "end_time": "2021-06-06T13:19:55.472171",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-06T13:19:35.841575",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
